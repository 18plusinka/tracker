#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ª–æ–≥–æ–≤
–ü–∞—Ä—Å–∏—Ç –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ª–æ–≥-—Ñ–∞–π–ª—ã —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
"""

import re
from datetime import datetime
from collections import Counter, defaultdict
import json

class LogAnalyzer:
    def __init__(self):
        self.log_patterns = {
            'apache': r'(\S+) \S+ \S+ \[(.*?)\] "(.*?)" (\d+) (\d+)',
            'nginx': r'(\S+) - - \[(.*?)\] "(.*?)" (\d+) (\d+) "(.*?)" "(.*?)"',
            'common': r'(\S+) - - \[(.*?)\] "(.*?)" (\d+) (\d+)',
            'combined': r'(\S+) - - \[(.*?)\] "(.*?)" (\d+) (\d+) "(.*?)" "(.*?)"',
            'custom': r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) (\w+): (.+)'
        }
        
        self.status_codes = {
            200: 'OK', 404: 'Not Found', 500: 'Server Error',
            301: 'Moved', 302: 'Redirect', 403: 'Forbidden'
        }
    
    def parse_log_file(self, filename, log_format='auto'):
        """–ü–∞—Ä—Å–∏—Ç –ª–æ–≥-—Ñ–∞–π–ª"""
        entries = []
        
        try:
            with open(filename, 'r', encoding='utf-8', errors='ignore') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line or line.startswith('#'):
                        continue
                    
                    entry = self.parse_log_line(line, log_format)
                    if entry:
                        entry['line_number'] = line_num
                        entries.append(entry)
            
            return entries
            
        except FileNotFoundError:
            print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {filename}")
            return []
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
            return []
    
    def parse_log_line(self, line, log_format='auto'):
        """–ü–∞—Ä—Å–∏—Ç –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É –ª–æ–≥–∞"""
        if log_format == 'auto':
            # –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞
            for format_name, pattern in self.log_patterns.items():
                match = re.match(pattern, line)
                if match:
                    return self.extract_fields(match, format_name, line)
        else:
            pattern = self.log_patterns.get(log_format)
            if pattern:
                match = re.match(pattern, line)
                if match:
                    return self.extract_fields(match, log_format, line)
        
        return {'raw_line': line, 'parsed': False}
    
    def extract_fields(self, match, format_name, raw_line):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª—è –∏–∑ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è"""
        groups = match.groups()
        entry = {'raw_line': raw_line, 'parsed': True, 'format': format_name}
        
        if format_name in ['apache', 'nginx', 'common', 'combined']:
            entry.update({
                'ip': groups[0],
                'timestamp': self.parse_timestamp(groups[1]),
                'request': groups[2],
                'status': int(groups[3]) if groups[3].isdigit() else 0,
                'size': int(groups[4]) if groups[4].isdigit() else 0
            })
            
            # –ü–∞—Ä—Å–∏–º HTTP –∑–∞–ø—Ä–æ—Å
            request_parts = groups[2].split()
            if len(request_parts) >= 2:
                entry['method'] = request_parts[0]
                entry['path'] = request_parts[1]
                entry['protocol'] = request_parts[2] if len(request_parts) > 2 else 'HTTP/1.1'
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –¥–ª—è nginx/combined
            if format_name in ['nginx', 'combined'] and len(groups) > 5:
                entry['referer'] = groups[5] if groups[5] != '-' else None
                entry['user_agent'] = groups[6] if len(groups) > 6 else None
        
        elif format_name == 'custom':
            entry.update({
                'timestamp': self.parse_timestamp(groups[0]),
                'level': groups[1],
                'message': groups[2]
            })
        
        return entry
    
    def parse_timestamp(self, timestamp_str):
        """–ü–∞—Ä—Å–∏—Ç –≤—Ä–µ–º–µ–Ω–Ω—É—é –º–µ—Ç–∫—É"""
        timestamp_formats = [
            '%d/%b/%Y:%H:%M:%S %z',  # Apache format
            '%d/%b/%Y:%H:%M:%S',     # Apache without timezone
            '%Y-%m-%d %H:%M:%S',     # Custom format
            '%Y-%m-%d %H:%M:%S.%f'   # With microseconds
        ]
        
        for fmt in timestamp_formats:
            try:
                return datetime.strptime(timestamp_str, fmt)
            except ValueError:
                continue
        
        return None
    
    def analyze_entries(self, entries):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∑–∞–ø–∏—Å–∏ –ª–æ–≥–æ–≤"""
        if not entries:
            return {}
        
        parsed_entries = [e for e in entries if e.get('parsed', False)]
        
        analysis = {
            'total_entries': len(entries),
            'parsed_entries': len(parsed_entries),
            'parse_rate': len(parsed_entries) / len(entries) * 100 if entries else 0
        }
        
        if parsed_entries:
            # –ê–Ω–∞–ª–∏–∑ IP –∞–¥—Ä–µ—Å–æ–≤
            ips = [e['ip'] for e in parsed_entries if 'ip' in e]
            analysis['top_ips'] = dict(Counter(ips).most_common(10))
            analysis['unique_ips'] = len(set(ips))
            
            # –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç—É—Å –∫–æ–¥–æ–≤
            status_codes = [e['status'] for e in parsed_entries if 'status' in e and e['status']]
            analysis['status_codes'] = dict(Counter(status_codes).most_common())
            
            # –ê–Ω–∞–ª–∏–∑ HTTP –º–µ—Ç–æ–¥–æ–≤
            methods = [e['method'] for e in parsed_entries if 'method' in e]
            analysis['http_methods'] = dict(Counter(methods).most_common())
            
            # –ê–Ω–∞–ª–∏–∑ –ø—É—Ç–µ–π
            paths = [e['path'] for e in parsed_entries if 'path' in e]
            analysis['top_paths'] = dict(Counter(paths).most_common(10))
            
            # –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–º–µ—Ä–æ–≤ –æ—Ç–≤–µ—Ç–æ–≤
            sizes = [e['size'] for e in parsed_entries if 'size' in e and e['size']]
            if sizes:
                analysis['response_sizes'] = {
                    'total_bytes': sum(sizes),
                    'avg_size': sum(sizes) / len(sizes),
                    'max_size': max(sizes),
                    'min_size': min(sizes)
                }
            
            # –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–∏
            timestamps = [e['timestamp'] for e in parsed_entries if 'timestamp' in e and e['timestamp']]
            if timestamps:
                analysis['time_range'] = {
                    'start': min(timestamps),
                    'end': max(timestamps),
                    'duration': str(max(timestamps) - min(timestamps))
                }
                
                # –ê–Ω–∞–ª–∏–∑ –ø–æ —á–∞—Å–∞–º
                hours = [t.hour for t in timestamps]
                analysis['traffic_by_hour'] = dict(Counter(hours).most_common())
        
        return analysis
    
    def detect_anomalies(self, entries):
        """–û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –∞–Ω–æ–º–∞–ª–∏–∏ –≤ –ª–æ–≥–∞—Ö"""
        anomalies = []
        
        for entry in entries:
            if not entry.get('parsed', False):
                continue
            
            # 4xx –∏ 5xx –æ—à–∏–±–∫–∏
            if entry.get('status', 0) >= 400:
                anomalies.append({
                    'type': 'error_status',
                    'severity': 'high' if entry['status'] >= 500 else 'medium',
                    'message': f"HTTP {entry['status']} from {entry.get('ip', 'unknown')}",
                    'entry': entry
                })
            
            # –ë–æ–ª—å—à–∏–µ —Ä–∞–∑–º–µ—Ä—ã –æ—Ç–≤–µ—Ç–æ–≤ (>10MB)
            if entry.get('size', 0) > 10 * 1024 * 1024:
                anomalies.append({
                    'type': 'large_response',
                    'severity': 'medium',
                    'message': f"Large response {entry['size']} bytes",
                    'entry': entry
                })
            
            # –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –ø—É—Ç–∏
            suspicious_patterns = [
                r'\.php$', r'admin', r'wp-admin', r'\.env',
                r'backup', r'config', r'\.git'
            ]
            
            path = entry.get('path', '')
            for pattern in suspicious_patterns:
                if re.search(pattern, path, re.IGNORECASE):
                    anomalies.append({
                        'type': 'suspicious_path',
                        'severity': 'high',
                        'message': f"Suspicious path access: {path}",
                        'entry': entry
                    })
                    break
        
        return anomalies
    
    def generate_report(self, analysis, anomalies):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –∞–Ω–∞–ª–∏–∑–∞"""
        report = []
        report.append("üìä –û–¢–ß–ï–¢ –ê–ù–ê–õ–ò–ó–ê –õ–û–ì–û–í")
        report.append("=" * 50)
        
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        report.append(f"\nüìà –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        report.append(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {analysis.get('total_entries', 0)}")
        report.append(f"–†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: {analysis.get('parsed_entries', 0)}")
        report.append(f"–ü—Ä–æ—Ü–µ–Ω—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: {analysis.get('parse_rate', 0):.1f}%")
        
        # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ IP
        if 'unique_ips' in analysis:
            report.append(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö IP: {analysis['unique_ips']}")
        
        # –í—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω
        if 'time_range' in analysis:
            time_range = analysis['time_range']
            report.append(f"–ü–µ—Ä–∏–æ–¥: {time_range['start']} - {time_range['end']}")
        
        # –¢–æ–ø IP –∞–¥—Ä–µ—Å–∞
        if 'top_ips' in analysis:
            report.append(f"\nüåê –¢–û–ü IP –ê–î–†–ï–°–ê")
            for ip, count in list(analysis['top_ips'].items())[:5]:
                report.append(f"  {ip}: {count} –∑–∞–ø—Ä–æ—Å–æ–≤")
        
        # –°—Ç–∞—Ç—É—Å –∫–æ–¥—ã
        if 'status_codes' in analysis:
            report.append(f"\nüìä –°–¢–ê–¢–£–° –ö–û–î–´")
            for status, count in analysis['status_codes'].items():
                status_name = self.status_codes.get(status, 'Unknown')
                report.append(f"  {status} ({status_name}): {count}")
        
        # HTTP –º–µ—Ç–æ–¥—ã
        if 'http_methods' in analysis:
            report.append(f"\nüîó HTTP –ú–ï–¢–û–î–´")
            for method, count in analysis['http_methods'].items():
                report.append(f"  {method}: {count}")
        
        # –¢–æ–ø –ø—É—Ç–∏
        if 'top_paths' in analysis:
            report.append(f"\nüìÅ –ü–û–ü–£–õ–Ø–†–ù–´–ï –ü–£–¢–ò")
            for path, count in list(analysis['top_paths'].items())[:5]:
                report.append(f"  {path}: {count}")
        
        # –ê–Ω–æ–º–∞–ª–∏–∏
        if anomalies:
            report.append(f"\n‚ö†Ô∏è  –ê–ù–û–ú–ê–õ–ò–ò ({len(anomalies)})")
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø–∞–º
            anomaly_types = defaultdict(list)
            for anomaly in anomalies:
                anomaly_types[anomaly['type']].append(anomaly)
            
            for anomaly_type, items in anomaly_types.items():
                report.append(f"  {anomaly_type}: {len(items)} —Å–ª—É—á–∞–µ–≤")
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ
                for item in items[:3]:
                    report.append(f"    - {item['message']}")
                
                if len(items) > 3:
                    report.append(f"    ... –∏ –µ—â–µ {len(items) - 3}")
        
        return "\n".join(report)

def main():
    analyzer = LogAnalyzer()
    print("=== –ê–ù–ê–õ–ò–ó–ê–¢–û–† –õ–û–ì–û–í ===\n")
    
    while True:
        print("1. –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ª–æ–≥-—Ñ–∞–π–ª")
        print("2. –ü–æ–∫–∞–∑–∞—Ç—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã")
        print("3. –¢–µ—Å—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–æ–∫–∏")
        print("4. –í—ã—Ö–æ–¥")
        
        choice = input("\n–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ: ")
        
        if choice == "1":
            filename = input("–ü—É—Ç—å –∫ –ª–æ–≥-—Ñ–∞–π–ª—É: ").strip()
            if not filename:
                print("‚ùå –ù–µ —É–∫–∞–∑–∞–Ω —Ñ–∞–π–ª")
                continue
            
            print("–§–æ—Ä–º–∞—Ç—ã: auto, apache, nginx, common, combined, custom")
            log_format = input("–§–æ—Ä–º–∞—Ç (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é auto): ").strip() or "auto"
            
            print("üîÑ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –ª–æ–≥-—Ñ–∞–π–ª...")
            entries = analyzer.parse_log_file(filename, log_format)
            
            if entries:
                print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–æ–∫: {len(entries)}")
                
                # –ê–Ω–∞–ª–∏–∑
                analysis = analyzer.analyze_entries(entries)
                anomalies = analyzer.detect_anomalies(entries)
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
                report = analyzer.generate_report(analysis, anomalies)
                print(f"\n{report}")
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
                save_report = input("\n–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—Ç—á–µ—Ç –≤ —Ñ–∞–π–ª? (y/n): ")
                if save_report.lower() == 'y':
                    report_filename = input("–ò–º—è —Ñ–∞–π–ª–∞: ") or "log_analysis_report.txt"
                    with open(report_filename, 'w', encoding='utf-8') as f:
                        f.write(report)
                    print(f"‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_filename}")
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ JSON
                save_json = input("–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ JSON? (y/n): ")
                if save_json.lower() == 'y':
                    json_data = {
                        'analysis': analysis,
                        'anomalies': anomalies,
                        'entries_sample': entries[:100]  # –ü–µ—Ä–≤—ã–µ 100 –∑–∞–ø–∏—Å–µ–π
                    }
                    
                    json_filename = input("–ò–º—è JSON —Ñ–∞–π–ª–∞: ") or "log_analysis.json"
                    with open(json_filename, 'w', encoding='utf-8') as f:
                        json.dump(json_data, f, ensure_ascii=False, indent=2, default=str)
                    print(f"‚úÖ JSON —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {json_filename}")
            else:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–∞–π–ª")
        
        elif choice == "2":
            print("\nüìã –ü–û–î–î–ï–†–ñ–ò–í–ê–ï–ú–´–ï –§–û–†–ú–ê–¢–´")
            print("=" * 40)
            print("apache  - Apache access log")
            print("nginx   - Nginx access log")
            print("common  - Common Log Format")
            print("combined- Combined Log Format")
            print("custom  - Custom format (timestamp level message)")
            print("auto    - –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞")
        
        elif choice == "3":
            log_line = input("–í–≤–µ–¥–∏—Ç–µ —Å—Ç—Ä–æ–∫—É –ª–æ–≥–∞: ").strip()
            if log_line:
                entry = analyzer.parse_log_line(log_line)
                print(f"\nüîç –†–ï–ó–£–õ–¨–¢–ê–¢ –ü–ê–†–°–ò–ù–ì–ê")
                print("=" * 30)
                
                if entry.get('parsed'):
                    print(f"‚úÖ –†–∞—Å–ø–æ–∑–Ω–∞–Ω —Ñ–æ—Ä–º–∞—Ç: {entry.get('format', 'unknown')}")
                    for key, value in entry.items():
                        if key not in ['raw_line', 'parsed', 'format']:
                            print(f"{key}: {value}")
                else:
                    print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ñ–æ—Ä–º–∞—Ç")
        
        elif choice == "4":
            break
            
        print("\n" + "="*40 + "\n")

if __name__ == "__main__":
    main()
